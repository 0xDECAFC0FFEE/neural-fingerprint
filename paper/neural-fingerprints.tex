\documentclass{article}
\usepackage{include/nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[noend]{algpseudocode}
\renewcommand\algorithmicthen{}
\renewcommand\algorithmicdo{}
\usepackage{algorithm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{array,booktabs}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\definecolor{myfavblue}{rgb}{0.1176, 0.392, 1.0}
\hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}

\newcommand{\vw}{\mathbf{w}}
\newcommand{\vx}{\mathbf{x}}
\newcommand{\vy}{\mathbf{y}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}

\title{Neural Molecular Fingerprints}%Learning Molecular Fingerprints:\\Convolutional Networks on Graphs}

\author{
David Duvenaud\\
Harvard University\\
\texttt{dduvenaud@seas.harvard.edu}
\And
Dougal Maclaurin\\
Harvard University\\
\texttt{maclaurin@physics.harvard.edu}
\And
Ryan P. Adams\\
Harvard University\\
\texttt{rpa@seas.harvard.edu}
}

%\nipsfinalcopy % Uncomment for camera-ready version
\begin{document}
\maketitle

\begin{abstract}
Predicting properties of molecules requires functions that take graphs as inputs.
Molecular graphs are usually processed using off-the-shelf hash-based functions which compute fixed-size fingerprint vectors, which are then fed into standard machine learning methods.
We introduce a generalization of commonly-used molecular fingerprints based on convolutional neural networks which allow learning of the entire feature pipeline.
\end{abstract}

\section{Introduction}

Much progress has recently been made by the application of deep learning to virtual screening, where the task is to predict the properties of novel molecules by generalizing from examples.
One difficulty with this task is that the input to the predictor, a molecule, can be of arbitrary size and shape.
Most machine learning pipelines can only handle inputs of a fixed size.
The current state of the art is to use off-the-shelf fingerprint software to compute fixed-dimensional feature vectors, using those as inputs to fully-connected deep neural networks or other standard machine learning methods.
This formula was followed by \citet{unterthinerdeep}, \citet{dahl2014multi}, \citet{tingley2014towards}, \citet{unterthiner2015toxicity},  \citet{ramsundar2015massively}, and \citet{ma_qsar_2015}.
The networks are optimized using gradient descent, but the molecular fingerprints and treated as fixed.
The state of the art in molecular fingerprints are extended-connectivity circular fingerprints, (ECFP)~\citep{ECFP2010}.%, a refinement of Morgan fingerprints.

In this paper, we replace the bottom layer of this stack, the fixed circular molecular fingerprints, with a differentiable neural network whose inputs are graphs representing the original molecule.
In these graphs, vertices represent individual atoms, and edges represent bonds.
The lower layers of these neural networks are convolutional in the sense that the same local filter is applied to each neighborhood of an atoms and its neighbors.
After several such layers, a global pooling step combines features from all the atoms in the network.
Figure \ref{fig:architecture sketch} shows a sketch of the net architecture.

\begin{figure}
\centerline{\includegraphics[width=0.4\textwidth]{figures/3d-nets/net1}
\includegraphics[width=0.4\textwidth]{../../DeepMoleculesData/experiments/2015-06-01-fig1/1/fingerprint_computation_schematic.pdf}
}
\caption{A visual representation of the neural fingerprint architecture.}
\label{fig:architecture sketch}
\end{figure}

Neural fingerprints offer several advantages over fixed, classical fingerprints:
\begin{itemize}
\item Molecular features can be automatically tuned to the task at hand.
Currently, many hand-engineered variants of circular fingerprints exist for various settings. [cite, and give examples].
\item Because circular fingerprints use a hash function to distinguish features, these fingerprints cannot encode any notion of similarity between fragments.
The tiniest difference between two fragments will result in their activation of completely different fingerprint indices.
\item By adapting to the task at hand, machine-optimized fingerprints can provide substantially better predictive performace.
We compare the effectiveness of neural fingerprints against standard fingerprints at predicting toxicity prediction, solubility prediction, drug efficacy, and organic LED efficiency.
\item Fixed fingerprints must be extremely large to encode all possible substructures without overlap.
For example, \cite{unterthinerdeep} used a fingerprint vector of size 43,000 for each molecule, after having removed rarely-occurring features in order to make the problem computationally feasible.
We show that better performance can be achieved using feature vectors a fraction of the size, reducing memory requirements.
\item Finally, the architecture presented here opens the door for more sophisticated information-aggregation architectures, such as recursive neural networks~\citep{socher2011semi, socher2011dynamic}.
\end{itemize}


\section{Circular fingerprints}

Circular fingerprints~\citep{glem2006circular} are a refinement of the Morgan algorithm~\citep{morgan1965generation}, designed to capture information about which substructures are present in a molecule in a way that is invariant to atom-relabeling.

Circular fingerprints generate each layer's features by applying a fixed hash function to the concatenated features of the neighborhood in the previous layer.
The result of these hashes are then treated as integer indices, where a 1 is written to the feature vector at the index given by the feature vector at each node in the graph.
Ignoring collisions, each index denotes the presence a particular molecular substructure.
The size of the substructures represented by each index depends on the depth of the network.
Thus the number of layers is referred to as the `radius' of the fingerprints.

The space of possible network architectures is large, but in the spirit of starting from a known-good configuration, we chose an architecture closely analogous to existing fingerprints.
Our neural fingerprints could reasonably be said to generalize existing fingerprints.
In Section \ref{sec:random is equivalent}, we show that randomly-initialized neural fingerprints have similar performance to existing fingerprints.

Standard circular fingerprints already implement a similar architecture, but the analogy to convolutional neural networks is not obvious.

\section{Creating a differentiable fingerprint}
This section describes our replacement of each discrete operation in circular fingerprints with a differentiable analogue.

\paragraph{Hashing}
The purpose of the hashing function applied at each layer of circular fingerprints is to combine information about an atom and its neighboring substructures.
This ensures that any change in a fragment, no matter how small, will lead to a different fingerprint index being activated.
We replace this hash operation with a single layer of a neural network, mapping from the concatenation of the features of an atom and his neighbors to a new feature vector for that atom.
This function is differentiable, and allows the computed result to be similar if the local molecular structure varies in irrelevant ways.

\paragraph{Indexing}
Circular fingerprints use an indexing operation to combine each atom's feature vector into a fingerprint.
Each atom sets a single bit of the fingerprint to one, at an index determined by the hash of its feature vector.
This pooling operation converts an arbitrary-sized graph into a fixed-sized vector.
For small molecules and a large choice of fingerprint length, the fingerprints are always sparse.
We use the \texttt{softmax} operation as a differentiable analogue of indexing.
In essence, each atom is asked to classify itself as belonging to a single category.
The sum of all these classification label vectors produces the final fingerprint.
In this model, each fingerprint has a specific meaning - arbitrary rotations of the weight matrices will not give rise to equivalent models.
This operation is analogous to the pooling operation in standard convolutional neural networks.
For both circular and neural fingerprints, each layer writes to the same output fingerprint vector.

\paragraph{Canonicalization}
Circular fingerprints produce the same fingerprints regardless of the ordering of atoms in each neighborhood, by sorting the neighboring atoms according to their features (and bond features).
We experimented with this sorting scheme, and also with applying the local feature transform on all possible permutation of the local neighborhood.
In the interests of simplicity and scalability, we decided to simply sum the local atom and bond features.

Algorithms \ref{alg:ecfp} and \ref{alg:neural} summarize these two algorithms and highlight their differences.

\begin{figure*}[t]
 \begin{minipage}[t]{0.49\columnwidth}
 \begin{algorithm}[H]
\caption{Circular fingerprints} 
\label{alg:ecfp} 
\begin{algorithmic}[1]
\State \textbf{Input:} {molecule, radius $R$, fingerprint length $S$}
\State \textbf{Initialize:} {fingerprint vector $\vf \leftarrow \vzero_S$}
\For {each atom $a$ in molecule} 
    \State $\vr_a \leftarrow g(a)$ \Comment {lookup atom features}
\EndFor
\For {$L = 1$ to $R$} \Comment {for each layer}
	\For {each atom $a$ in molecule}
		\State $\vr_{n1} \dots \vr_{nN} = \textnormal{neighbors}(a)$
		\State $\vv \leftarrow [\vr_a \vr_{n1} \dots, \vr_{nN}]$ \Comment {concatenate}
		\State $\vr_a \leftarrow \textnormal{hash}(\vv)$ \Comment {hash function}
		\State $i \leftarrow \textnormal{mod}(r_a, S)$ \Comment {convert to index}		
		\State $\vf_{i} \leftarrow 1$ \Comment {Write 1 at index}
	\EndFor
\EndFor
\State \textbf{Return:} {binary vector $\vf$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\columnwidth}
\begin{algorithm}[H]
\caption{Neural fingerprints} 
\label{alg:neural} 
\begin{algorithmic}[1]
\State \textbf{Input:} {molecule, radius $R$, {\color{myfavblue} hidden weights $H_1 \dots H_R$, output weights $W_1 \dots W_R$}}
\State \textbf{Initialize:} {fingerprint vector $\vf \leftarrow \vzero_S$}
\For {each atom $a$ in molecule} 
	\State $\vr_a \leftarrow g(a)$ \Comment {lookup atom features}
\EndFor
\For {$L = 1$ to $R$} \Comment {for each layer}
    \For {each atom $a$ in molecule}
		\State $\vr_{n1} \dots \vr_{nN} = \textnormal{neighbors}(a)$
		\State $\vv \leftarrow [\vr_a \vr_{n1}, \dots \vr_{nN}]$ \Comment {concatenate}
		%\State $\vr_a \leftarrow {\color{myfavblue} \vf_{\theta_L}}(v)$ \Comment {{\color{myfavblue} smooth function}}
		\State $\vr_a \leftarrow {\color{myfavblue} \sigma(H_L \times \vv)}$ \Comment {{\color{myfavblue} smooth function}}
		\State $\vi \leftarrow {\color{myfavblue} \textnormal{softmax}(W_L \times \vr_a)}$ \Comment {{\color{myfavblue} sparsify}}
		\State $\vf \leftarrow {\color{myfavblue} \vf + \vi}$ \Comment {{\color{myfavblue}add to fingerprint}}
    \EndFor
\EndFor
\State \textbf{Return:} { {\color{myfavblue} real-valued} vector $\vf$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\caption{Pseudocode of circular fingerprints (\emph{left}) and neural fingerprints (\emph{right}).
Differences are highlighted in blue.
Every non-differentiable operation has been replaced with a differentiable analogue.}
\end{figure*}


\section{Related work}
This work is similar in spirit to the Neural Turing Machine~\citep{graves2014neural}, in the sense that we take an existing discrete computational architecture, and make each part differentiable in order to do gradient-based optimization.

\paragraph{Neural fingerprints}
The most closely related work is \citet{lusci2013deep}, who build a neural network having graph-valued inputs.
Their approach is to remove all cycles and build the graph into a tree structure, choosing one atom to be the root.
A recursive neural network~\citep{socher2011dynamic, socher2011semi} is then run from the leaves to the root to produce a fixed-size representation.
Because a graph having $N$ nodes has $N$ possible roots, all $N$ possible graphs are constructed.
The final descriptor is a sum of the representations computed by all distinct graphs.
There are as many distinct graphs as there are atoms in the network.
The computational cost of this method thus grows as $\mathcal{O}(F^2N^2)$, where $F$ is the size of the feature vector and $N$ is the number of atoms, making it less suitable for large molecules.

\paragraph{Convolutional Neural Networks}
Convolutional neural networks have been used to model images, speech, and time series~\citep{lecun1995convolutional}.
However, standard convolutional architectures use a fixed computational graph, making them difficult to apply to objects of varying size or structure, such as molecules.
More recently, \cite{KalchbrennerACL2014} and others have developed a convolutional neural network architecture for modeling sentences of varying length.

\paragraph{Neural nets for quantitative structure-activity relationship (QSAR)}
The modern standard for predicting properties of novel molecules is to compose circular fingerprints with fully-connected neural networks or other regression methods.
\cite{dahl2014multi, ma_qsar_2015} used circular fingerprints as inputs to an ensemble of neural networks, Gaussian processes, and random forests.
\cite{ramsundar2015massively} used circular fingerprints (of depth 2) as inputs to a multitask neural network, showing that multiple tasks helped performance.

%\paragraph{Machine learning for identifying promising molecules}
%\cite{Eckert2007225, bergeron2011modeling} provide reviews of the field.
%\cite{tingley2014towards} used a variety of standard machine learning algorithms to predict the photovoltaic efficiency of organic molecules.

\paragraph{Neural Networks on Graphs}
\cite{graphnn2009} propose a neural network model for graphs having an unusual training procedure.
The forward pass consists of running a message-passing scheme to equilibrium, a fact which allows the reverse-mode gradient to be computed without storing the entire forward computation.
They apply their network to predicting mutagenesis of molecular compounds, as well as web page rankings.
\cite{micheli2009neural} also propose a neural network model for graphs with a learning scheme whose inner loop optimizes not the training loss, but rather the correlation between each newly-proposed vector and the training error residual.
They apply their model to a dataset of boiling points of 150 molecular compounds.
Our paper builds on these ideas in the following ways:
Our method replaces their complex training algorithms with simple gradient-based optimization, generalizes existing molecular fingerprint computations, and applies these networks in the context of modern QSAR pipelines which use neural networks on top of the fingerprints to increase model capacity.

\cite{bruna2013spectral} introduce convolutional networks on graphs in the regime where the graph structure is fixed, and each training example differs only in having different features at the vertices of the same graph.
In contrast, our networks address the situation where each training input is a different graph.

\paragraph{Unrolled inference algorithms}
\citet{hershey2014deep} and others have noted that iterative inference procedures sometimes resemble the feedforward computation of a recurrent neural network.
One natural combination of these ideas is to parameterize these inference steps, and train a neural network to approximately match the output of exact inference in a smaller number of iterations.
The neural fingerprint, when viewed in this light, resembles an unrolled message-passing algorithm on the original graph.


\section{Experiments}

\subsection{Randomly initialized neural fingerprints are similar to circular fingerprints}
\label{sec:random is equivalent}
We argue that existing Circular fingerprints are similar to neural fingerprints with large, randomly-initialized parameters.
In the limit of large parameter values, \texttt{tanh} nonlinearities approach step functions, and the \texttt{softmax} operator approaches a one-hot encoded \texttt{argmax} operator.

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{../../DeepMoleculesData/experiments/2015-05-04-compare-fingerprints/91-try-again/morg_conv_corr.pdf}}
\caption{A visual representation of the neural fingerprint architecture.
}
\label{fig:fingerprint similarity}
\end{figure}

Figure \ref{fig:fingerprint similarity} shows a scatterplot of the similarity between fingerprints of different modecules, generated by ECFPs and neural fingerprints.
Similarity was measured using a continuous generalization of the Tanimoto or Jacard similarity measure, given by
\begin{equation}
\textnormal{similarity}(\vx, \vy) = \frac{\sum \min(x_i, y_i)}{\sum \max(x_i, y_i)}
\end{equation}
Both fingerprint lengths were set to 2048.
The line of points on the far left shows that for some pairs of molecules, binary ECFP fingerprints have exactly zero overlap.

\subsection{Examining learned features}
In contrast to circular fingerprints, which can each only be activated by a single fragment of a single radius, neural fingerprints can be activated by variations of the same structure, making them more parsimonious and interpretable.

\paragraph{Solubility features}
%
\newcommand{\molfeature}[2]{\includegraphics[width=3.5cm]{../../DeepMoleculesData/experiments/2015-05-30-visualize-fps/1/figures/fp_#1_highlight_#2.pdf}}%
\begin{figure}[h]
\begin{tabular}{>{\centering}m{1in} >{\centering}m{3.5cm} >{\centering}m{3.5cm} >{\centering\arraybackslash}m{3.5cm}}
Fragments most activated by strongest pro-solubility fingerprint element & \molfeature{15}{0} & \molfeature{15}{3} & \molfeature{15}{2} \\
\midrule
Fragments most activated by strongest anti-solubility fingerprint element & \molfeature{18}{4} & \molfeature{18}{1} & \molfeature{18}{2}
\end{tabular}
\caption{Visualizing fingerprints optimized for predicting solubility.
Shown here are representative samples of molecular fragments (highlighted in blue) which most activate each fingerprint.
Shown here are the two fingerprints with the strongest predictive weights.
In contrast to circular fingerprints, which can each only be activated by a single fragment of a single radius, neural fingerprints can be activated by variations of the same structure, making them more parsimonious and interpretable.}
\label{fig:learned features solubility}
\end{figure}
%
Figure \ref{fig:learned features solubility} shows which fragments maximally activated various features.
These fingerprints were trained as the input to a linear model predicting the solubility as measured in the Delaney dataset~\citep{delaney_data_2004}.
Shown are the two fingerprints with the strongest predictive weights.
The first fingerprint has a positive predictive relationship with solubility, and is most activated by fragments containing an OH group.
The second fingerprint, which is strongly negatively predictive of solubility is activated by non-polar repeated ring structures.

\paragraph{Toxicity features}
%
\newcommand{\molfeaturetox}[2]{\includegraphics[width=3.5cm]{../../DeepMoleculesData/experiments/2015-05-30-visualize-fps/4-toxic/figures/fp_#1_highlight_#2.pdf}}%
\begin{figure}[h]
\begin{tabular}{>{\centering}m{1in} >{\centering}m{3.5cm} >{\centering}m{3.5cm} >{\centering\arraybackslash}m{3.5cm}}
Fragments most activated by strongest toxicity fingerprint element & \molfeaturetox{1}{6} & \molfeaturetox{1}{4} & \molfeaturetox{1}{5}
\end{tabular}
\caption{Visualizing fingerprints optimized for predicting toxicity.
Shown here are representative samples of molecular fragments (highlighted in red) which most activate each fingerprint.}
\label{fig:learned features toxicity}
\end{figure}
%
Figure \ref{fig:learned features toxicity} shows which fragments maximally activated various features.
We also ran the same experiment on data from the Tox 21 Data Challenge \citet{tox21}.
\citet{unterthiner2015toxicity} constructed similar visualizations by searching for molecules that most activated neurons in a deep neural network on top of fixed circular fingerprints.
However, given fixed fingerprints, it's not clear how to automatically find what fragments are relevant to the output.
\citet{unterthiner2015toxicity} did this in a semi-manual way: in order to determine which fragments of the molecules were predictive of toxicity, they searched over a list of substructures already known to be toxic (toxicphores) to find the toxic substructure that most correlated with the given neuron.

In contrast, our vizualizations are generated automatically, without the need to restrict the range of possible answers beforehand.

\subsection{Experimental setup}
Our pipeline takes as input the SMILES~\citep{weininger1988smiles} string-based encoding of each molecule, which is then converted into a graph using RDkit~\citep{rdkit}.
We also used RDkit to produce circular fingerprints for our baseline experiments.
To optimize hyperparameters, we used Whetlab~(\url{www.whetlab.com}), a commercial implementation of Bayesian optimization~\citep{snoek2012practical}.
We optimized the learning and architecture parameters of both the Circular fingerprint network and the neural fingerprint network using 400 trials.

In our convolutional networks, the initial atom and bond features were chosen to be similar to those used by ECFP.
The atom features were a concatenation of a one-hot encoding of the element, the degree, the number of attached hydrogen atoms, and the implicit valence, and one feature indicating if the atom is in an aromatic ring. 
The bond features were a concatenation of whether the bond type was single, double, triple, or aromatic, whether the bond was conjugated, and whether the bond is part of a ring.

Initial experiments included optional batch normalization~\citep{ioffe2015batch}.
Because batch normalization consistently led to better validation errors on both models, we eventually made it mandatory.
We also experimented with \texttt{tanh} vs \texttt{relu} activation functions for both the neural fingerprint net and the fully-connected nets.  \texttt{relu} had a slight but consistent performance advantage on the validation set.
We also experimented with dropconnect~\citep{wan2013regularization}, a variant of dropout in which weights are randomly set to zero instead of hidden units, but it led to worse validation error in general.

We optimized for 1000 minibatches of size 100 using the Adam algorithm~\citep{Adam14}, a variant of RMSprop that includes momentum.
Adam's $\beta_1$ and $\beta_2$ parameters were included in the Bayesian optimization.

Table \ref{table:bayesopt params} gives a full listing of all parameters optimized using Bayesian optmization for both models.

\begin{table}
\center
\begin{tabular}{l|lcl}
Hyperparameter & Min & Max & Type \\
\hline
Fingerprint length & 10  & 2048 & integer \\
Fingerprint depth  & 0   & 6    & integer \\
Log initial scale  & -6  & -2   & real \\
Log learn rate     & -7  & -2   & real \\
Log $\beta_1$      & -8  & 2    & real \\
Log $\beta_2$      & -6  & 2    & real \\
Log $L_2$ penalty  & -12 & -1   & real \\
Hidden layer size  & 10  & 500  & integer \\
\hline 
Convolution width  & 2   & 50   & integer
\end{tabular}
\caption{Hyperparameter ranges used in Bayesian optimization.
Both circular and neural fingerprints shared all hyperparameter ranges.
Neural fingerprints have one additional hyperparameter: The number of features stored at each node in the graph.}
\label{table:bayesopt params}
\end{table}

\subsection{Datasets}
We compared the performance of standard circular fingerprints against our neural fingerprints on a variety of domains:
%
\begin{itemize}
\item {\bf Solubility:} \cite{delaney_data_2004} measured the aqueous solubility of 1144 molecules.
\item{\bf Drug efficacy:} \citet{gamo2010thousands} measured the half maximal effective concentration (EC$_{50}$) {\it in vitro} of 20,000 molecules against a sulfide-resistant strain of {\it P. falciparum}.
\item {\bf Toxicity:} The \citet{tox21} released several assays measuring the toxicity of thousands of compounds.
We measured performance on the MMP Stress response panel, whose predictive target was a binary response.
\item {\bf Organic photovoltaic efficiency:} The Harvard Clean Energy Project~\citet{hachmann2011harvard} has run millions of molecules through a TD-DFT simulation to estimate the photovoltaic efficiency of organic molecules.
Each simulation requires tens of CPU hours, meaning that predicting outcome of the simulation can greatly speed up the virtual screening process.
\end{itemize}
%
To examine the applicability of neural fingerprints to the relatively small size of datasets commonly encountered, we subsampled 1000 points from each dataset.

\subsection{Predictive accuracy}
Results are summarized in Table~\ref{table:main results}.

\begin{table}
\begin{tabular}{lll|cc}
Predicted property              & Dataset                     & Metric & Circular        & Neural   \\
\hline
Solubility (log Mol/L)          & \citet{delaney_data_2004}   &   RMSE & 1.04 $\pm$ 0.06 & \bf{0.72} $\pm$ 0.05 \\
Drug efficacy (EC$_{50}$ in nM) & \citet{gamo2010thousands}   &   RMSE & \bf{1.17} $\pm$ 0.12 & \bf{1.15} $\pm$ 0.16  \\
Toxicity (Binary)               & \citet{tox21}               &   NLL  & ?               &      ?               \\
Photovoltaic efficiency (\%)    & \citet{hachmann2011harvard} &  RMSE  & 1.96 $\pm$ 0.3  & \bf{1.68} $\pm$ 0.15
\end{tabular}
\label{table:main results}
\caption{Predictive accuracy of neural fingerprints compared to standard circular fingerprints.}
\end{table}

In general, the circular fingerprints chose a 


\subsection{Implementation Details}
Automatic differentiation (AD) software packages such as
Theano~\citep{Bastien-Theano-2012, bergstra2010scipy} significantly speed up development time by providing gradients automatically, but can only handle limited control structures and indexing.
Since we required relatively complex control flow and indexing in order to implement variants of Algorithm \ref{alg:neural}, we used a more flexible automatic differentiation package for Python, Autograd\footnote{\url{github.com/HIPS/autograd}}.
This package differentiates standard Numpy~\citep{oliphant2007python} code, and can differentiate code containing while loops, branches, and indexing.

Code for all experiments in this paper will be made available upon publication.

\section{Limitations}
In this section we detail some limitations of the neural fingerprint architecture used in this paper.

\paragraph{Computational cost}
Neural fingerprints have the same asymptotic complexity in the number of atoms and the depth of the network as circular fingerprints, but have additional terms due to the matrix multiplies necessary to transform the feature vector at each step.
To be precise, computing the neural fingerprint of depth $R$, fingerprint length $L$ of a molecule with $N$ atoms using a molecular convolutional net having $F$ features at each layer costs $\mathcal{O}(RNFL + RNF^2)$.
In practice, training neural networks on top of circular fingerprints usually took several minutes, while training both the fingerprints and the network on top took on the order of an hour.

\paragraph{Limited computation at each layer}
How complicated should we make the function that goes from one layer of the network to the next?
In this paper we chose the simplest feasible architecture: a single layer of a neural network.
However, it may be fruitful to apply multiple layers of nonlinearities between each message-passing step (as in \cite{graphnn2009}), or to make information preservation easier by adapting the Long Short-Term Memory~\citep{hochreiter1997long} architecture to pass information upwards.

\paragraph{Limited information propagation across the graph}
The local message-passing architecture developed in this paper scales well asymptotically in the size of the graph, but its ability to propagate information across the graph is limited by the depth of the network.
This may be appropriate for small graphs such as those representing the small organic molecules used in this paper.
However, in the worst case, it will take a depth $\frac{N}{2}$ network to distinguish between all possible graphs of size $N$.
To avoid this problem, \citet{bruna2013spectral} proposed a hierarchical clustering of graph substructures.
A tree-structure network could examine the structure of the entire graph using only $\log(N)$ layers, but would require learning to parse molecules.
Techniques from natural language processing~\citep{tai2015improved} might be fruitfully adapted to this domain.

%\paragraph{Inapplicability to novel domains without retraining}
%Circular fingerprints have a useful property: their feature dimension is implicitly infinite.
%Because each possible substructure always maps to 


%\paragraph{Preserve asymmetries explicitly or implicitly}
%If we tie the weights of all neighboring vertices, then ordering information is lost locally, although it is still preserved implicitly in the relations between nodes in the next layer.

%\paragraph{3D features}
%How to extend this to using 3d features?

%\subsection{Interpretability}
%[Idea: Use nested dropout to allow a variable-sized descriptor.]
%Explain that it's analogous to PCA for neural nets

%\paragraph{Using LSTMs instead of RNNs}
%Since \cite{hochreiter1997long}


\section{Conclusions}
Machine-optimized features have replaced hand-crafted features in speech recognition, machine vision, and natural-language processing.
We generalized existing hand-crafted molecular feature pipelines to allow the improvement of these pipelines, as well as their customization for diverse tasks.
By taking care that each operation in the generalized feature pipeline be differentiable, we can use standard neural-network training methods to scalably optimize the parameters of these neural molecular fingerprints.

%\section*{Acknowledgments}
\bibliography{references.bib}
\bibliographystyle{include/icml2015}
\end{document}