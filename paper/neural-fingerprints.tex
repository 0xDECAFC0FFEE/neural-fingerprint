\documentclass{article}
\usepackage{include/nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[noend]{algpseudocode}
\renewcommand\algorithmicthen{}
\renewcommand\algorithmicdo{}
\usepackage{algorithm}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{array,booktabs}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue}

\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}

\title{Neural Molecular Fingerprints}

\author{
David Duvenaud\\
Harvard University\\
\texttt{dduvenaud@seas.harvard.edu}
\And
Dougal Maclaurin\\
Harvard University\\
\texttt{maclaurin@physics.harvard.edu}
\And
Ryan P. Adams\\
Harvard University\\
\texttt{rpa@seas.harvard.edu}
}

%\nipsfinalcopy % Uncomment for camera-ready version
\begin{document}
\maketitle

\begin{abstract}
Predicting properties of molecules requires functions that take graphs as inputs.
Molecular graphs are usually processed using off-the-shelf hash-based functions which compute fixed-size fingerprint vectors, which are then fed into standard machine learning methods.
We introduce a generalization of commonly-used molecular fingerprints based on convolutional neural networks which allow learning of the entire feature pipeline.
\end{abstract}

\section{Introduction}

Much progress has recently been made by the application of deep learning to virtual screening, where the task is to predict the properties of novel molecules by generalizing from examples.
One difficulty with this task is that the input to the predictor, a molecule, can be of arbitrary size and shape.
Most machine learning pipelines can only handle inputs of a fixed size.
The current state of the art is to use off-the-shelf fingerprint software to compute fixed-dimensional feature vectors, using those as inputs to fully-connected deep neural networks or other standard machine learning methods.
This formula was followed by \citet{unterthinerdeep}, \citet{dahl2014multi}, \citet{tingley2014towards}, \citet{unterthiner2015toxicity},  \citet{ramsundar2015massively}, and \citet{ma_qsar_2015}.
The networks are optimized using gradient descent, but the molecular fingerprints and treated as fixed.
The state of the art in molecular fingerprints are extended-connectivity circular fingerprints, (ECFP)~\citep{ECFP2010}.%, a refinement of Morgan fingerprints.

In this paper, we replace the bottom layer of this stack, the fixed circular molecular fingerprints, with a differentiable neural network whose inputs are graphs representing the original molecule.
In these graphs, vertices represent individual atoms, and edges represent bonds.
The lower layers of these neural networks are convolutional in the sense that the same local filter is applied to each neighborhood of an atoms and its neighbors.
After several such layers, a global pooling step combines features from all the atoms in the network.
Figure \ref{fig:architecture sketch} shows a sketch of the net architecture.

\begin{figure}
\centerline{\includegraphics[width=0.4\textwidth]{figures/3d-nets/net1}
\includegraphics[width=0.4\textwidth]{../../DeepMoleculesData/experiments/2015-06-01-fig1/1/fingerprint_computation_schematic.pdf}
}
\caption{A visual representation of the neural fingerprint architecture.}
\label{fig:architecture sketch}
\end{figure}

Neural fingerprints offer several advantages over fixed, classical fingerprints:
\begin{itemize}
\item Molecular features can be automatically tuned to the task at hand.
Currently, many hand-engineered variants of circular fingerprints exist for various settings. [cite, and give examples]
\item By adapting to the task at hand, machine-optimized fingerprints can provide substantially better predictive performace.
We compare the effectiveness of neural fingerprints against standard fingerprints at predicting toxicity prediction, solubility prediction, drug efficacy, and organic LED efficiency.
\item Fixed fingerprints must be extremely large to encode all possible substructures without overlap.
For example, \cite{unterthinerdeep} used a fingerprint vector of size 43,000 for each molecule, after having removed rarely-occurring features in order to make the problem computationally feasible.
We show that better performance can be achieved using feature vectors a fraction of the size, reducing memory requirements.
\item Finally, the architecture presented here opens the door for more sophisticated information-aggregation architectures, such as recursive neural networks~\citep{socher2011semi, socher2011dynamic}.
\end{itemize}

By generalizing the state-of-the-art extended-connectivity circular fingerprints , we allow


\section{Connection to circular fingerprints}

%Circular fingerprints are designed to capture information about which substructures are present in a molecule in a way that is invariant to atom-relabeling.

Standard circular fingerprints already implement a similar architecture, but the analogy to convolutional neural networks is not obvious.
Circular fingerprints generate each layer's features by applying a fixed hash function to the concatenated features in each neighborhood.
The result of these hashes are then treated as integer indices, where a 1 is written to the feature vector at the index given by the feature vector at each node in the graph.
If there are no collisions in the hash function, then each index represents a particular molecular substructure.
Because the size of the substructures represented by each index depend on the depth of the network, the number of layers is referred to as the `radius' of the fingerprints.

The space of possible network architectures is large, but in the spirit of starting from a known-good configuration, we chose an architecture closely analogous to existing fingerprints.
Our neural fingerprints could reasonably be said to generalize existing fingerprints.
In Section \ref{sec:random is equivalent}, we show that randomly-initialized neural fingerprints have similar performance to existing fingerprints.

\section{Creating a differentiable fingerprint}
This section describes our replacement of each discrete operation in circular fingerprints with a differentiable analogue.

\paragraph{Hashing}
The most straightforward choice in our design is that of replacing the hashing function applied at each neighborhood with a single layer of a neural network.
This raises several design decisions, such as the choice of activation function, and the dimension of the feature vectors.
We make these choices in the standard way, using Bayesian optimization [cite] to optimize cross-validation accuracy.

\paragraph{Indexing}
One striking design aspect of circular fingerprints is the transformation of a feature vector into an index.
In convolutional neural networks, pooling is usually performed on each feature separately.
[more connecting text]
We use the \texttt{softmax} operation as a differentiable analogue of indexing.
We do this for three reasons: To match the structure of Circular fingerprints 2. To ensure sparsity of the fingerprint 3. So that each fingerprint has a specific meaning - arbitrary rotations of the weight matrices will not give rise to equivalent models.

\paragraph{Canonicalization}
Circular fingerprints produce the same fingerprints regardless of the ordering of atoms in each neighborhood, by sorting the neighboring atoms according to their features (and bond features).
We experimented with this sorting scheme, and also with applying the local feature transform on all possible permutation of the local neighborhood.
In the interests of simplicity and scalability, we decided to simply sum the local atom and bond features.

Algorithms \ref{alg:ecfp} and \ref{alg:neural} summarize these two algorithms and highlight their differences.

\begin{figure*}[t]
 \begin{minipage}[t]{0.49\columnwidth}
 \begin{algorithm}[H]
\caption{Circular fingerprints} 
\label{alg:ecfp} 
\begin{algorithmic}[1]
\State \textbf{Input:} {molecule, radius $R$, size $S$}
\State \textbf{Initialize:} {fingerprint vector $\vf \leftarrow \vzero_S$}
\For {each atom $a$ in molecule} 
    \State $\vr_a \leftarrow g(a)$ \Comment {lookup atom features}
\EndFor
\For {$L = 1$ to $R$} \Comment {for each layer}
	\For {each atom $a$ in molecule}
		\State $\vr_{n1} \dots \vr_{nN} = \textnormal{neighbors}(a)$
		\State $\vv \leftarrow [\vr_a \vr_{n1} \dots, \vr_{nN}]$ \Comment {concatenate}
		\State $\vr_a \leftarrow \textnormal{hash}(\vv)$ \Comment {hash function}
		\State $i \leftarrow \textnormal{mod}(r_a, S)$ \Comment {convert to index}		
		\State $\vf_{i} \leftarrow 1$ \Comment {Write 1 at index}
	\EndFor
\EndFor
\State \textbf{Return:} {binary vector $\vf$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.49\columnwidth}
\begin{algorithm}[H]
\caption{Neural fingerprints} 
\label{alg:neural} 
\begin{algorithmic}[1]
\State \textbf{Input:} {molecule, radius $R$, size $S$, {\color{blue} $\theta_1 \dots \theta_R$}}
\State \textbf{Initialize:} {fingerprint vector $\vf \leftarrow \vzero_S$}
\For {each atom $a$ in molecule} 
	\State $\vr_a \leftarrow g(a)$ \Comment {lookup atom features}
\EndFor
\For {$L = 1$ to $R$} \Comment {for each layer}
    \For {each atom $a$ in molecule}
		\State $\vr_{n1} \dots \vr_{nN} = \textnormal{neighbors}(a)$
		\State $\vv \leftarrow [\vr_a \vr_{n1}, \dots \vr_{nN}]$ \Comment {concatenate}
		\State $\vr_a \leftarrow {\color{blue} \vf_{\theta_L}}(v)$ \Comment {{\color{blue} smooth function}}
		\State $\vi \leftarrow \textnormal{{\color{blue}softmax}}(\vr_a)$ \Comment {{\color{blue} sparsify}}
		\State $\vf \leftarrow {\color{blue} \vf + \vi}$ \Comment {{\color{blue}add to fingerprint}}
    \EndFor
\EndFor
\State \textbf{Return:} { {\color{blue} real-valued} vector $\vf$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\caption{Pseudocode of circular fingerprints (\emph{left}) and neural fingerprints (\emph{right}).
Differences are highlighted in blue.
Every non-differentiable operation has been replaced with a differentiable analogue.}
\end{figure*}




\section{Related work}

\paragraph{Neural fingerprints}
The most closely related work is \citet{lusci2013deep}, who also replace circular fingerprints with a neural network having graph-valued inputs.
Their approach is to remove all cycles and build the graph into a tree structure, choosing one atom to be the root.
A recursive neural network~\citep{socher2011dynamic, socher2011semi} is then run from the leaves to the root to produce a fixed-size representation.
The final descriptor is a sum of the representations of all distinct graphs.
There are as many distinct graphs as there are atoms in the network.
The computational cost of this method thus grows as $\mathcal{O}(F^2N^2)$, where $F$ is the size of the feature vector and $N$ is the number of atoms.


\paragraph{Convolutional Neural Networks}

Convolutional neural networks have been used to model images, speech, and time series~\cite{lecun1995convolutional}.
However, standard convolutional architectures use a fixed computational graph, making them difficult to apply to objects of varying size or structure, such as molecules.
More recently, \cite{KalchbrennerACL2014} and others have developed a convolutional neural network architecture for modeling sentences of varying length.


\paragraph{Neural nets for QSAR}

\cite{ramsundar2015massively} Massively multitask networks for drug discovery

\cite{dahl2014multi, ma_qsar_2015} used standard deep neural networks, and didn't do any feature engineering.

\paragraph{Machine learning for identifying promising molecules}

\cite{Eckert2007225, bergeron2011modeling} provide reviews of the field.
\cite{tingley2014towards} used a variety of standard machine learning algorithms to predict the photovoltaic efficiency of organic molecules.

\paragraph{Neural Networks on Graphs}

\cite{graphnn2009} The Graph Neural Network Model

\cite{micheli2009neural} Neural network for graphs

\paragraph{Unrolled inference algorithms}
\citet{hershey2014deep} and others have noted that iterative inference procedures sometimes resemble the feedforward computation of a recurrent neural network.
One natural combination of these ideas is to parameterize these inference steps, and train a neural network to approximately match the output of exact inference in a smaller number of iterations.
The neural fingerprint, when viewed in this light, resembles an unrolled message-passing algorithm on the original graph.


\section{Experiments}


\subsection{Randomly initialized neural fingerprints are similar to circular fingerprints}
\label{sec:random is equivalent}

We argue that existing Circular fingerprints are similar to neural fingerprints with large, randomly-initialized parameters.
In the limit of large parameter values, \texttt{tanh} nonlinearities approach step functions, and the softmax operator approaches the \texttt{argmax} operator.

\begin{figure}[h]
\centerline{\includegraphics[width=0.5\textwidth]{../../DeepMoleculesData/experiments/2015-05-04-compare-fingerprints/91-try-again/morg_conv_corr.pdf}}
\caption{A visual representation of the neural fingerprint architecture.
}
\label{fig:fingerprint similarity}
\end{figure}

Figure \ref{fig:fingerprint similarity} shows a scatterplot of the similarity between fingerprints of different modecules, generated by ECFPs and neural fingerprints.
Similarity was measured using a continuous generalization of the Tanimoto similarity measure [cite].
Both fingerprint lengths were set to 2048.
The line of points on the far left shows that for some pairs of molecules, binary ECFP fingerprints have exactly zero overlap.



\subsection{Examining learned features}

In contrast to circular fingerprints, which can each only be activated by a single fragment of a single radius, neural fingerprints can be activated by variations of the same structure, making them more parsimonious and interpretable.

\paragraph{Solubility features}
%
\newcommand{\molfeature}[2]{\includegraphics[width=3.5cm]{../../DeepMoleculesData/experiments/2015-05-30-visualize-fps/1/figures/fp_#1_highlight_#2.pdf}}%
\begin{figure}[h!]
\begin{tabular}{>{\centering}m{1in} >{\centering}m{3.5cm} >{\centering}m{3.5cm} >{\centering\arraybackslash}m{3.5cm}}
Fingerprint 15 Solubility coefficient 0.22 & \molfeature{15}{0} & \molfeature{15}{3} & \molfeature{15}{2} \\
\midrule
Fingerprint 18 Solubility coefficient -0.83 & \molfeature{18}{4} & \molfeature{18}{1} & \molfeature{18}{2}
\end{tabular}
\caption{Visualizing fingerprints optimized for predicting solubility.
Shown here are representative samples of molecular fragments (highlighted in blue) which most activate each fingerprint.
Shown here are the two fingerprints with the strongest predictive weights.
In contrast to circular fingerprints, which can each only be activated by a single fragment of a single radius, neural fingerprints can be activated by variations of the same structure, making them more parsimonious and interpretable.}
\label{fig:learned features solubility}
\end{figure}
%
Figure {fig:learned features solubility} shows which fragments maximally activated various features.
These fingerprints were trained as the input to a linear model predicting the solubility as measured in the Delaney dataset~\citep{delaney_data_2004}.
Shown are the two fingerprints with the strongest predictive weights.
The first fingerprint has a positive predictive relationship with solubility, and is most activated by fragments containing an OH group.
The second fingrprint, which is strongly negatively predictive of solubility is activated by non-polar repeated ring structures.




\paragraph{Toxicity features}
%
\newcommand{\molfeaturetox}[2]{\includegraphics[width=3.5cm]{../../DeepMoleculesData/experiments/2015-05-30-visualize-fps/4-toxic/figures/fp_#1_highlight_#2.pdf}}%
\begin{figure}[h!]
\begin{tabular}{>{\centering}m{1in} >{\centering}m{3.5cm} >{\centering}m{3.5cm} >{\centering\arraybackslash}m{3.5cm}}
Fingerprint 15 Solubility coefficient 0.22 & \molfeaturetox{1}{6} & \molfeaturetox{1}{4} & \molfeaturetox{1}{5}
\end{tabular}
\caption{Visualizing fingerprints optimized for predicting solubility.
Shown here are representative samples of molecular fragments (highlighted in blue) which most activate each fingerprint.
Shown here are the two fingerprints with the strongest predictive weights.
In contrast to circular fingerprints, which can each only be activated by a single fragment of a single radius, neural fingerprints can be activated by variations of the same structure, making them more parsimonious and interpretable.}
\label{fig:learned features toxicity}
\end{figure}
%
Figure {fig:learned features toxicity} shows which fragments maximally activated various features.
We also ran the same experiment on data from the Tox 21 Data Challenge \citet{tox21}.
\citet{unterthiner2015toxicity} constructed similar visualizations by searching for molecules that most activated neurons in a deep neural network on top of fixed circular fingerprints.
However, given fixed fingerprints, it's not clear how to automatically find what fragments are relevant to the output.
\citet{unterthiner2015toxicity} did this in a semi-manual way: in order to determine which fragments of the molecules were predictive of toxicity, they searched over a list of substructures already known to be toxic (toxicphores) to find the toxic substructure that most correlated with the given neuron.

In contrast, our vizualizations are generated automatically, without the need to restrict the range of possible answers beforehand.




\subsection{Experimental setup}

Our pipeline takes as input the SMILES~\citep{weininger1988smiles} string-based encoding of each molecule, which is then converted into a graph using RDkit~\citep{rdkit}.
We also used RDkit to produce circular fingerprints for our baseline experiments.
To optimize hyperparameters, we used Whetlab~(\url{www.whetlab.com}, a commercial implementation of Bayesian optimization~\citep{snoek2012practical}.
We optimized the learning and architecture parameters of both the Circular fingerprint network and the neural fingerprint network using 400 trials.

In our convolutional networks, the initial atom and bond features were chosen to be similar to those used by ECFP.
The atom features were a concatenation of a one-hot encoding of the element, the degree, the number of attached hydrogen atoms, and the implicit valence, and one feature indicating if the atom is in an aromatic ring. 
The bond features were a concatenation of whether the bond type was single, double, triple, or aromatic, whether the bond was conjugated, and whether the bond is part of a ring.

Initial experiments included optional batch normalization~\citep{ioffe2015batch}.
Because batch normalization consistently led to better validation errors on both models, we eventually made it mandatory.
We also experimented with \texttt{tanh} vs \texttt{relu} activation functions for both the neural fingerprint net and the fully-connected nets.  \texttt{relu} had a slight but consistent performance advantage on the validation set.
We also experimented with dropconnect~\citep{wan2013regularization}, a variant of dropout in which weights are randomly set to zero instead of hidden units, but it led to worse validation error in general.

We optimized for 1000 minibatches of size 100 using the Adam algorithm~\citep{Adam14}, a variant of RMSprop that includes momentum.
Adam's $\beta_1$ and $\beta_2$ were included in the Bayesian optimization.

\begin{table}
\center
\begin{tabular}{c|ccc}
Hyperparameter & Mininum & Maximum & Type \\
\hline
fingerprint length & 10   &   2048  & integer \\
fp depth  & 0 &       6 &     integer \\
log init scale & -6 &      -2 &     float \\
log learn rate & -7 &      -2 &    float \\
log b1 & -8 &      2 &    float \\
log b2 & -6 &     2 &   float \\
log l2 penalty & -12 &     -1 &   float \\
h1 size & 10   &   500  & integer \\
\hline 
convolution width & 2 & 50 & integer
\end{tabular}
\end{table}



\subsection{Datasets}

Tripod dataset: \url{https://tripod.nih.gov/tox21/challenge/}
\cite{unterthiner2015toxicity}

Delaney dataset:
\cite{delaney_data_2004}


\subsection{Predictive accuracy}

Results are summarized in Table~\ref{table:main results}.

%conv mean: 1.70349097683 conv std err 0.173207307432
%morg mean: 1.85368888753 morg std err 0.0980490688708

\begin{table}
\begin{tabular}{ccc|cc}
Predicted property           & Dataset                     & Metric & Circular        & Neural   \\
\hline
Solubility (log Mol/L)       & \citet{delaney_data_2004}   &   RMSE & 1.04 $\pm$ 0.06 & \bf{0.72} $\pm$ 0.05 \\
Drug efficacy                & ?                           &   AUC  & ?               &      ?               \\
Toxicity                     & \citet{tox21}               &    ?   & ?               &      ?               \\
Photovoltaic efficiency (\%) & \citet{hachmann2011harvard} &  RMSE  & 1.85 $\pm$ 0.10 & \bf{1.70} $\pm$ 0.09
\end{tabular}
\label{table:main results}
\caption{Predictive accuracy of neural fingerprints compared to standard circular fingerprints.}
\end{table}


\subsection{Implementation Details}
Automatic differentiation (AD) software packages such as
Theano~\citep{Bastien-Theano-2012, bergstra2010scipy} significantly speed up development time by providing gradients automatically, but can only handle limited control structures and indexing.
Since we required relatively complex control flow and indexing in order to implement variants of Algorithm \ref{alg:neural}, we used the relatively flexible automatic differentiation package for Python, available at \url{github.com/HIPS/autograd}.
This package differentiates standard Numpy~\citep{oliphant2007python} code, and can differentiate code containing while loops, branches, indexing, and even its own gradient evaluations.

Code for all experiments in this paper will be made available upon publication.

\section{Future extensions}
\paragraph{Performing more computation at each layer}
How complicated should we make the function that goes from one layer of the graph to the next?
\paragraph{Learning to parse molecules}
The local message-passing architecture developed in this paper

%\paragraph{Preserve asymmetries explicitly or implicitly}
%If we tie the weights of all neighboring vertices, then ordering information is lost locally, although it is still preserved implicitly in the relations between nodes in the next layer.

%\paragraph{3D features}
%How to extend this to using 3d features?

%\subsection{Interpretability}
%[Idea: Use nested dropout to allow a variable-sized descriptor.]
%Explain that it's analogous to PCA for neural nets

\paragraph{Using LSTMs instead of RNNs}


\section{Conclusions}
Machine-optimized features have replaced hand-crafted features in speech recognition, machine vision, and natural-language processing.
We generalized existing hand-crafted molecular feature pipelines to allow the improvement of these pipelines, as well as their customization for diverse tasks.
By taking care that each operation in the generalized feature pipeline be differentiable, we can use standard neural-network training methods to scalably optimize the parameters of these neural molecular fingerprints.

%\section*{Acknowledgments}
\bibliography{references.bib}
\bibliographystyle{include/icml2015}
\end{document}