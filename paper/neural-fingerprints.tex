\documentclass{article} % For LaTeX2e
\usepackage{include/nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[noend]{algpseudocode}
\renewcommand\algorithmicthen{}
\renewcommand\algorithmicdo{}
\usepackage{algorithm}
\usepackage{natbib}
\usepackage{graphicx}

\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\hypersetup{ %
    pdftitle={},
    pdfauthor={},
    pdfsubject={},
    pdfkeywords={},
    pdfborder=0 0 0,
    pdfpagemode=UseNone,
    colorlinks=true,
    linkcolor=mydarkblue,
    citecolor=mydarkblue,
    filecolor=mydarkblue,
    urlcolor=mydarkblue,
    pdfview=FitH}

\newcommand{\vw}{\mathbf{w}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\vf}{\mathbf{f}}
\newcommand{\vg}{\mathbf{g}}
\newcommand{\vi}{\mathbf{i}}
\newcommand{\vr}{\mathbf{r}}
\newcommand{\vzero}{\mathbf{0}}
\newcommand{\ones}[1]{\mat{1}_{#1}}
\newcommand{\eye}[1]{\mat{E}_{#1}}
\newcommand{\tra}{^{\mathsf{T}}}
\newcommand{\vect}[1]{{\bf{#1}}}
\newcommand{\mat}[1]{\mathbf{#1}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\npderiv}[2]{\nicefrac{\partial #1}{\partial #2}}

\title{Neural Molecular Fingerprints}


\author{
David Duvenaud\\
Harvard University\\
\texttt{dduvenaud@seas.harvard.edu}
\And
Dougal Maclaurin\\
Harvard University\\
\texttt{maclaurin@physics.harvard.edu}
\And
Ryan P. Adams\\
Harvard University\\
\texttt{rpa@seas.harvard.edu}
}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle


\begin{abstract}
Performing regression or classification on molecules requires finding functions which can take graph-valued inputs.
Such functions are usually built using fixed, hash-based functions to compute fingerprint vectors, which are then fed into standard machine learning methods.
We introduce a generalization of commonly-used molecular fingerprints based on convolutional neural networks which allow learning of the entire feature pipeline.
\end{abstract}

\section{Introduction}

\subsection{Desirable Properties of Molecular Representations}

\begin{itemize}
\item invariant to global orientation
\item invariant to atom re-labeling
\item invariant to the SMILES representation (at the very least)
\item NOT invariant to reordering locally (maintain information about the orientation)
    example:  fragment1 - C - O - fragment2 is not the same as fragment1 - O - C - fragment2 
   \item speed
\end{itemize}

\section{Dealing with local ordering}

Morgan fingerprints deal with local ordering by having a canonical order.



\section{Extended-connectivity fingerprints}

\begin{figure}
\centerline{\includegraphics[width=0.4\textwidth]{figures/3d-nets/net1}}
\caption{A visualization of one of the convolutional nets constructed on a molecule.}
\label{placeholder}
\end{figure}

\begin{figure}
\centerline{
\includegraphics[width=0.2\textwidth]{figures/convnet-features/hidden-unit-0}
\includegraphics[width=0.2\textwidth]{figures/convnet-features/hidden-unit-1}}
\caption{The molecular fragments which most activate two of the features of the convolutional net.}
\label{placeholder}
\end{figure}




\begin{figure*}
 \begin{minipage}[t]{0.5\columnwidth}
 \begin{algorithm}[H]
\caption{Circular fingerprints} 
\label{alg:ecfp} 
\begin{algorithmic}[1]
\State \textbf{Input:} {molecule, radius $R$, size $S$}
\State \textbf{Initialize:} {fingerprint vector $\vf \leftarrow \vzero_S$}
\For {each atom $a$ in molecule} 
    \State $\vr_a \leftarrow g(a)$ \Comment {lookup atom features}
\EndFor
\For {$L = 1$ to $R$} \Comment {for each layer}
	\For {each atom $a$ in molecule}
		\State $\vr_{n1} \dots \vr_{nN} = \textnormal{neighbors}(a)$
		\State $\vv \leftarrow [\vr_a, \vr_{n1} \dots \vr_{nN}]$ \Comment {concatenate}
		\State $\vr_a \leftarrow \textnormal{hash}(\vv)$ \Comment {hash function}
		\State $i \leftarrow \textnormal{mod}(r_a, S)$ \Comment {convert to index}		
		\State $\vf_{i} \leftarrow 1$ \Comment {Write 1 at index}
	\EndFor
\EndFor
\State \textbf{Return:} {binary vector $\vf$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\begin{minipage}[t]{0.5\columnwidth}
\begin{algorithm}[H]
\caption{Neural fingerprints} 
\label{alg:neural} 
\begin{algorithmic}[1]
\State \textbf{Input:} {molecule, radius $R$, size $S$, {\color{blue} $\theta_1 \dots \theta_R$}}
\State \textbf{Initialize:} {fingerprint vector $\vf \leftarrow \vzero_S$}
\For {each atom $a$ in molecule} 
	\State $\vr_a \leftarrow g(a)$ \Comment {lookup atom features}
\EndFor
\For {$L = 1$ to $R$} \Comment {for each layer}
    \For {each atom $a$ in molecule}
		\State $\vr_{n1} \dots \vr_{nN} = \textnormal{neighbors}(a)$
		\State $\vv \leftarrow [\vr_a, \vr_{n1} \dots \vr_{nN}]$ \Comment {concatenate}
		\State $\vr_a \leftarrow {\color{blue} \vf_{\theta_L}}(v)$ \Comment {{\color{blue} smooth function}}
		\State $\vi \leftarrow \textnormal{{\color{blue}softmax}}(\vr_a)$ \Comment {{\color{blue} sparsify}}
		\State $\vf \leftarrow {\color{blue} \vf + \vi}$ \Comment {{\color{blue}add to fingerprint}}
    \EndFor
\EndFor
\State \textbf{Return:} { {\color{blue} real-valued} vector $\vf$}
\end{algorithmic}
\end{algorithm}
\end{minipage}
\hfill
\caption{Pseudocode of circular fingerprints (\emph{left}) and neural fingerprints (\emph{right}).
Differences are highlighted in blue.
Every non-differentiable operation has been replaced with a differentiable analogue.}
\end{figure*}



\section{Related work}

\paragraph{Fingerprints}

[Cite Morgan] and \cite{ECFP2010} developed extended circular fingerprints.
These fingerprints map identical molecules to the same set of fingerprints.
These same fingerprints have been pressed into service as a measure of similarity.

However, for a meaningful measure of similarity, 

\paragraph{Convolutional Neural Networks}

Convolutional neural networks have been used to model images, speech, and time series\cite{lecun1995convolutional}.
However, standard convolutional architectures use a fixed computational graph, making them difficult to apply to objects of varying size or structure, such as molecules.
More recently, \cite{KalchbrennerACL2014} developed a convolutional neural network architecture for modeling sentences of varying length and structure.

\paragraph{Recursive Neural Networks}

\cite{socher2011semi} and \cite{socher2011dynamic} use a pyramidal architecture for performing inference on variable-length sentences.

\paragraph{Neural nets for QSAR}

\cite{dahl2014multi} used standard deep neural networks, and didn't do any feature engineering.

\paragraph{Machine learning for identifying promising molecules}

\cite{Eckert2007225, bergeron2011modeling} provide reviews of the field.
\cite{tingley2014towards} used a variety of standard machine learning algorithms to predict the photovoltaic efficiency of organic molecules.

\paragraph{Neural Networks on Graphs}

\cite{graphnn2009} The Graph Neural Network Model

\cite{micheli2009neural} Neural network for graphs



\section{Experiments}

[How much is conceptual purity worth?  We can try using only topology, or include lots of hand-engineered features that we think will be useful.]

\subsection{Are Morgan fingerprints basically a hash function for molecules?}

We can answer this question by evaluating the performance of neural fingerprints generated by randomly initialized weights.


Predicting molecular weight:

Mean predictor
Performance (RMSE):
Train: 241.630442201
Test:  243.609627269

Fingerprints with linear weights
Performance (RMSE):
Train: 142.774743361
Test:  151.932246613

Random net with linear weights
Performance (RMSE):
Train: 144.492288513
Test:  150.182390847



\subsection{Datasets}

\subsection{Interpretability}
[Idea: Use nested dropout to allow a variable-sized descriptor.]
Explain that it's analogous to PCA for neural nets

[Wishlist: Include figures showing which fragments maximally activate different features - hopefully showing that they correspond to interpretable, familiar concepts]


\section{Extensions}

\paragraph{How much computation to perform at each layer?}
i.e. how complicated should we make the function that goes from one layer of the graph to the next?

\paragraph{Building parse trees}
For the pyramidal architecture, we need to decide on a parse tree of the molecule.
One way to do this in a 'soft' way might be to max-pool over many different local parsings.
This could be done at multiple layers, which would limit the exponential blowup.

\paragraph{Preserve asymmetries explicitly or implicitly}
If we tie the weights of all neighboring vertices, then ordering information is lost locally, although it is still preserved implicitly in the relations between nodes in the next layer.


\section{Conclusions}


\section*{acknowledgments}
This work was partially supported by a grant from the Spanish Ministry of Science and Technology.



\subsubsection*{References}

\bibliography{references.bib}
\bibliographystyle{include/icml2015}



\end{document}